{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analítica de texto\n",
    "\n",
    "> _Preparado por: [Juan Javier Santos Ochoa](https://twitter.com/jjsantoso) ([LNPP](https://www.lnpp.mx/))_\n",
    "\n",
    "* En este tutorial veremos cómo hacer algunas tareas de analítica de texto usando métodos de aprendizaje no supervizado. La idea es obtener algo de conocimiento sobre estos texto sin tener necesidad que revisarlos extensivamente.\n",
    "\n",
    "* Las tareas más comunes dentro del análisis de texto incluyen categorización, clusterización, extracción de entidades, análisis de sentimiento, resumen de documentos, predicción de palabras y [generación automática de contenido](https://automatedinsights.com/)\n",
    "\n",
    "* Algunas de las aplicaciones incluyen, por ejemplo, calcular la [polaridad/sentimiento de tweets](https://www.inegi.org.mx/app/animotuitero/#/app/multiline), identificar el idioma de una publicación, detectar si una noticia menciona nombres de personas, lugares u otras características, identificar los tópicos de los que tratan un conjunto de documentos o [agrupar programas sociales similares](https://github.com/plataformapreventiva/social_programs_text_analysis) según la descripción de sus reglas de operación.\n",
    "\n",
    "* A un conjunto de datos para hacer análisis de texto se le llama corpus y cada texto individual se conoce como un documento.\n",
    "\n",
    "* Los modelos de ML no pueden recibir texto directamente, sino que cada documento se debe procesar de manera que tenga una representación vectorial numérica que pueda ser entendida por los algoritmos.\n",
    "\n",
    "* En esta sesión veremos cómo procesar un corpus y usar algoritmos usuales de ML para hacer algunas de las tareas comunes en analítica de texto.\n",
    "* Primero veremos algunas tareas comunes de preprocesamiento de texto y luego veremos algunas formas para representar un texto de forma vectorial.\n",
    "* Esta sesión está basada principalmente en el libro de Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, strip_accents_ascii\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from string import punctuation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib._color_data as mcd # paleta de colores\n",
    "from matplotlib import markers\n",
    "\n",
    "print(sklearn.__name__, sklearn.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(pd.__name__, pd.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discursos\n",
    "* Tomaremos los discursos de la Junta de Gobierno del Banco de México: https://www.banxico.org.mx/publicaciones-y-prensa/discursos/discursos-junta-gobierno-pala.html\n",
    "* Se omitieron los discursos en inglés.\n",
    "* Cada texto está limitado a las primeras 5000 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = pd.read_csv('datos/discursos.csv')\n",
    "discursos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "\n",
    "* Antes de entrenar cualquier modelo tenemos que preprocesar el texto para homogeneizarlo y remover algunas palabras o caracteres que pueden ser irrelevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minúsculas y remover acentos\n",
    "* Como nuestro texto está en un dataframe de pandas podemos usar el método `str.lower()` para convertir todo a minúsculas.\n",
    "* También podemos usar el método `.apply()` para aplicar una función personalizada. sklearn tiene la función `strip_accents_ascii`, dentro del módulo `sklearn.feature_extraction.text`, para remover acentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos['texto'] = discursos['texto'].str.lower().apply(strip_accents_ascii)\n",
    "discursos['texto'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "* Las stopwords son palabras muy comunes que usualmente no aportan mucha información. Estas palabras suelen ser artículos o preposiciones.\n",
    "* En español podemos usar la lista de stopwords que viene en la librería [`nltk`](https://www.nltk.org/), dentro del módulo `stopwords`. La libreríaa ya viene instalada en Anaconda.\n",
    "* Dependiendo de la aplicación. podemos agregar nuestras propias stopwords.\n",
    "* A continuación vamos a usar las stopwords de nltk que están guardadas en el archivo `datos/stopwords_spanol.csv` y creamos una función que las remueve de cualquier texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = pd.read_csv('datos/stopwords_spanol.csv')['stopwords'].tolist()\n",
    "def remueve_stopwords(texto):\n",
    "    p = [t for t in texto.lower().split() if t not in stops]\n",
    "    return ' '.join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos['texto'] = discursos['texto'].apply(remueve_stopwords)\n",
    "discursos['texto'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover puntuación y números\n",
    "\n",
    "* También puede ser útil remover signos de puntucación y números\n",
    "* Dentro de la librería `string` se encuentra la variable `punctuation` que contiene los principales signos de puntuación. En español debemos agregarle algunos signos de apertura (¡¿)\n",
    "* Usamos el método `.str.replace()` de las series de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos['texto'] = discursos['texto'].str.replace('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿\\d]', '')\n",
    "discursos['texto'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsa de palabras (Bag of Words)\n",
    "\n",
    "* BoW es una de las formas más simples de representar texto y usualmente se desempeña bastante bien en ciertas tareas, como categorización.\n",
    "* Consiste en contar la ocurrencia de todas las palabras en cada documento.\n",
    "* En BoW no importa la estructura gramatical del texto (oraciones, párrafos) ni el orden de las palabras, simplemente importa cuántas veces ocurren.\n",
    "* Para hacer una representación BoW de un corpus se necesitan tres pasos:\n",
    "    1. Tokenización: consiste en descomponer el texto en elementos más pequeños, por ejemplo oraciones o palabras.\n",
    "    2. Vocabulario: Se recolectan todas las palabras que aparecen en todos los documentos y se asigna un identificador numérico único a cada una, usualmente en orden alfabético.\n",
    "    3. Codificación: Para cada documento se cuenta la frecuencia de todas las palabras del vocabulario.\n",
    "\n",
    "* Con esta información se construye una matriz en la que cada fila representa un documento y las columnas una palabra del vocabulario:\n",
    "\n",
    "<img src=\"imagenes/bow_matriz.PNG\" width=600>\n",
    "\n",
    "\n",
    "* Veamos el siguiente ejemplo con solo 3 documentos:\n",
    "\n",
    "```python\n",
    "[\"Me gusta el café sin azucar y sin crema\", \"Desayuno café con leche y pan con fruta\", \"Solo tomo leche deslactosada\"]\n",
    "```\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>azucar</th>      <th>café</th>      <th>con</th>      <th>crema</th>      <th>desayuno</th>      <th>deslactosada</th>      <th>el</th>      <th>fruta</th>      <th>gusta</th>      <th>leche</th>      <th>me</th>      <th>pan</th>      <th>sin</th>      <th>solo</th>      <th>tomo</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>2</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>1</td>      <td>2</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>  </tbody></table>\n",
    "\n",
    "* Tenemos un vocabulario de 15 palabras.\n",
    "* Con esta representación ahora podemos usar algunos de los algoritmos de ML que hemos visto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de BOW\n",
    "\n",
    "* En sklearn está la función `CountVectorizer()` dentro del módulo `sklearn.feature_extraction.text` que nos genera una matriz con el conteo de cada palabra para cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"Me gusta el café sin azucar y sin crema\", \"Desayuno café con leche y pan con fruta\", \"Solo tomo leche deslactosada\"]\n",
    "vect = CountVectorizer()\n",
    "vect.fit_transform(frases).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos ver el vocabulario con el atributo `.vocabulary_`. Por defecto, todas las palabras son convertidas a minúsculas.\n",
    "* El vocabulario es un diccionario en el que el _key_ es cada palabra y el _value_ es un índice numérico basado en el orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "* Obtuvimos buenos resultados con el conteo de palabras, sin embargo, en general ocurrirá que en documentos más largos habrá un mayor conteo que en documentos más cortos, aunque pertenezcan a la misma categoría.\n",
    "\n",
    "* Una forma de controlar esto es dividiendo el número ocurrencias de cada palabra sobre el número total de palabras en el documento. Esto se conoce como term-frecuency, abreviado como tf.\n",
    "\n",
    "* Un problema de contar las frecuencias, ya sea en términos absolutos o relativos, es que muchas palabras comunes aparecen en muchos documentos, por tanto no son tan informativas. En cambio, hay otras palabras que aparecen con menor frecuencia en ciertos documentos pero que son más distintivas y por tanto aportan más información al modelo. Entonces, una forma de mejorar las predicciones puede ser modificando el peso de cada palabra de forma inversamente proporcional a su frecuencia en el corpus. Esto se conoce como inverse document frecuency, idf.\n",
    "\n",
    "* Al aplicar ambas trasnformaciones obtenemos tf-idf: “Term Frequency times Inverse Document Frequency”. La intuición es que al aplicar este método se le da más peso a las palabras que ocurren frecuentemente en ciertos documentos, pero no en muchos.\n",
    "\n",
    "* En sklearn podemos usar `TfidfVectorizer`. sklearn usa en particular la fórmula:\n",
    "\n",
    "$$\\text{tfidf}(w, d) = \\text{tf}\\log{(\\frac{N+1}{N_w +1})} + 1$$\n",
    "\n",
    "\n",
    " Donde: $N$ es el número de documentos en los datos de entrenamiento, N_w es el número de documentos en los datos de entrenamiento en los que aparece la palabra $w$ y tf es el número de veces que aparece la palabra $w$ en el documento $d$\n",
    " \n",
    " * Existe una opción `sublinear_tf` que si es activada (True) reemplaza tf por $1+\\log (\\text{tf})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transf = TfidfVectorizer()\n",
    "tf_transf.fit_transform(frases).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram vectorizer\n",
    "\n",
    "* Los N-gramas son combinaciones de N palabras contiguas juntas. Por ejemplo, en la frase \"A mí me gusta el café\" tenemos los siguientes bigramas:\n",
    "```python\n",
    "(\"a\", \"mí\"), (\"mí\", \"me\"), (\"me\", \"gusta\"), (\"gusta\", \"el\"), (\"el\", \"café\")\n",
    "```\n",
    "\n",
    "* Usualmente la unión de dos o tres palabras es más informativa que una sola palabra, por lo que podemos obtener mejores resultados en el caso que existan palabras con ambiguedad, lo que suele ser muy común en el análisis de sentimientos.\n",
    "\n",
    "* Tanto `CountVectorizer()` como `TfidfVectorizer()` tienen un parámetro `ngram_range` que debe ser una tupla especificando el número mínimo y máximo de N-gramas que se adicionarán. Por ejemplo, ngram_range=(1, 3) indica que se van a adicionar las palabras solas y todas las combinaciones de 2 y 3 palabras contiguas.\n",
    "\n",
    "* Una dificultad de usar N-gramas es que es más difícil encontrar la combinación de N-palabras contiguas. Por tanto no se suele especificar más allá de trigramas y se usa cuando se tiene un corpus suficientemente grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "pd.DataFrame(data=ngram_vectorizer.fit_transform(frases).toarray(), columns=ngram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitaciones de BoW y Tf-IDF\n",
    "* BoW no captura información secuencial. Documentos con diferentes significados pero iguales palabras tienen la misma representación:\n",
    "<img src=\"imagenes/bow_limitacion.PNG\" width=600>\n",
    "* No captura información semántica. Palabras que se escriben igual pero tienen diferentes significados se capturan igual. Por ejemplo, banco puede ser una institución o una silla.\n",
    "* Para superar estas dificultades es necesario usar modelos que puedan captar de mejor manera el caracter secuencial del texto y aprovechar el contexto para evitar ambiguedades. Algunos de estos modelos muy populares son [Word2vec](https://www.tensorflow.org/tutorials/text/word_embeddings) y las [Redes Neuronales Recurrentes](https://www.tensorflow.org/tutorials/text/text_classification_rnn) (RNN), pero están más allá del objetivo de este tutorial. \n",
    "* A continuación usaremos el Análisis semántico latente para intentar obtener representaciones que comprenden un poco mejor el contenido de un texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis semántico latente (LSA)\n",
    "\n",
    "* El análisis semántico latente es una técnica para reducir la dimensionalidad de una matriz de frecuencia de términos. Usa la técnica de descompsición de valores singulares, parecido a PCA, pero a diferencia de esta, LSA funciona mejor con datos esparsos y los datos no necesitan ser centrados alrededor de la media.\n",
    "* LSA permite representas cada documento en una dimensión mucho menor y además el resultado tiende a encontrar parecidos semánticos en los documentos, de tal manera que documentos que tratan sobre los mismos temas, aunque no usen las mismas palabras, tienden a tener representaciones más parecidas.\n",
    "\n",
    "* Suponiendo que tenemos una matriz $X$ en la que cada elemento $(i, j)$ describe la ocurrencia del término $j$ en el documento $i$. Con $m$ documentos y un vocabulario de tamaño $n$.\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "x_{1, 1} & \\cdots & & x_{1, n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "x_{i, 1} & x_{i, j} & & x_{i, n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "x_{m, 1} & \\cdots & & x_{m, n} \\\\\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "* Por la teoría de algebra lineal, existe una descomposición de valores singulares de la matrix $X$:\n",
    "\n",
    "    $$X = U\\Sigma V^T$$\n",
    "\n",
    "    Donde $U$ y $V$ son matrices ortogonales ($Q^TQ = QQ^T = I$) y $\\Sigma$ es una matriz diagonal. $U$ y $V$ contienen los llamados vectores propios izquierdos y derechos, respectivamente y los valores de la diagonal $\\Sigma$ se conocen como los valores propios.\n",
    "    \n",
    "* Si seleccionamos los $k$ valores propios más altos $\\Sigma_k$, junto con sus respectivos vectores propios $U_k$ y $V_k$, entonces obtenemos la aproximación de rango $k$ de $X$ con mínimo error:\n",
    "\n",
    "$$ X_k = U_k \\Sigma_k V^T_k$$\n",
    "\n",
    "* Esta aproximación tiene tamaño $m\\times k$, con $k\\leq n$.\n",
    "* Si queremos representar un nuevo documento $d_i$ de la forma reducida entonces aplicamos la operación:\n",
    " $$\\hat{d_i} = \\Sigma^{-1}_kV^T_k d_i$$\n",
    " \n",
    "* La representación del corpus en un espacio de menor dimensión usando descomposición de valores singulares genera lo que se conoce como un \"espacio semántico\".\n",
    "* Con el espacio de menor dimensión es posible desarrolar algunas tareas como:\n",
    "    * Clasificar.\n",
    "    * Comparar la similitud de documentos.\n",
    "    * Clusterizar.\n",
    "    * Encontrar palabras relacionadas.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "\n",
    "* En sklearn podemos hacer LSA aplicando la función `TruncatedSVD()` al resultado de una matriz de frecuencia de terminos `CountVectorizer()` o `TfidfVectorizer()`\n",
    "* Antes debemos preprocesar el texto.\n",
    "* En la función `TruncatedSVD()` debemos especificar cuál es el número de componentes al que queremos reducir la información de los documentos. Este número debe ser menor o igual al total de documentos.\n",
    "\n",
    "* El método `.fit_tranform()` ajusta el modelo y genera una matriz de tamaño $(n\\_documentos, n\\_componentes )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_transformer = Pipeline([\n",
    "                            ('tfidf', TfidfVectorizer(sublinear_tf=True)), \n",
    "                            ('svd', TruncatedSVD(n_components=100, n_iter=10, random_state=42))])\n",
    "\n",
    "svd_matrix = svd_transformer.fit_transform(discursos['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de documentos\n",
    "\n",
    "* Como cada documento está representado por un vector, podemos considerar la distancia entre los vectores como una medida de la similaridad de los documentos.\n",
    "* En el análisis de texto es usual usar una métrica conocidad como distancia coseno, $D_c$, que se deriva de la similitud coseno (cosine similarity).\n",
    "\n",
    "$$D_c(A, B) = 1 - S_c(A, B)$$\n",
    "$$S_c = \\frac{\\mathbf{A}\\cdot\\mathbf{B}}{||\\mathbf{A}||\\text{ }||\\mathbf{B}||}$$\n",
    "\n",
    "* En la gráfica anterior, la línea roja representa la distancia euclidiana, mientras que la azul representa la distancia coseno, esta última captura la dirección y es independiente de la magnitud de los vectores.\n",
    "* En sklearn usamos la función `pairwise_distances` dentro del módulo `sklearn.metrics` para calcular una matriz de distancias, en particular especificamos la distancia 'cosine'.\n",
    "* Le pasamos la matriz que contiene la representación vectorial de cada documento.\n",
    "* La matriz que resulta tiene todas las combinaciones de distancias de los documentos: La i-ésima fila de la matriz contiene la distancia del i-ésimo documento con todos los demás.\n",
    "* Para un documento, nos interesa encontrar los demás documentos con los que tienen menor distancia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pairwise_distances(svd_matrix, svd_matrix, metric='cosine')\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "index_query = 50\n",
    "discursos.loc[[index_query], 'titulo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hacemos una copia del DataFrame de los discursos y agregamos una columna con la distancia. De esta manera podemos ver cuál es el título y el texto de los documentos más parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist = discursos.copy()\n",
    "df_dist['dist'] = distance_matrix[index_query]\n",
    "df_dist.sort_values('dist').head()[['titulo', 'texto']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterización de documentos\n",
    "\n",
    "* Podemos usar la representación vectorial de los documentos como input para algún algoritmo de clusterización, como el de KMeans.\n",
    "* Podemos probar varios valores de n_clusters y verificar usando el método del codo o el coeficiente de silueta. Sin embargo, tratándose de texto, siempre es bueno revisar que los clústeres tinen sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_cluster = Pipeline([\n",
    "    ('svd_transform', svd_transformer),\n",
    "    ('kmeans', KMeans(n_clusters=5) )\n",
    "])\n",
    "\n",
    "svd_cluster.fit(discursos['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación, hacemos una copia del DataFrame original y creamos una nueva columna que contenga el cluster al cual se asignó cada texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos_cluster = discursos.copy()\n",
    "discursos_cluster['grupo'] = svd_cluster.predict(discursos['texto'])\n",
    "discursos_cluster.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos_cluster['grupo'].value_counts().plot.bar(title='Distribución de documentos por grupo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación revisamos los primeros 5 títulos de cada clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in discursos_cluster['grupo'].sort_values().unique():\n",
    "    print('\\n####### Grupo:', g, '#'*50)\n",
    "    for t in discursos_cluster.query('grupo==@g')['titulo'].head():\n",
    "        print(t.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del espacio de documentos\n",
    "\n",
    "* Puede ser muy útil ubicar todos los documentos, o una parte de ellos, en un plano y ver cómo se distribuyen los clústeres y qué tan cercanos están unos documentos a otros.\n",
    "* Sin embargo, cada documento está representado en un espacio de dimensión 100, lo que es imposible de visualizar directamente.\n",
    "* Para ello, existe una técnica de reducción de la dimensionalidad llamada tSNE específicamente diseñada para poder visualizar en un plano de 2 o 3 dimensiones.\n",
    "* En palabras generales, t-distributed Stochastic Neighbor Embedding (TSNE) intenta encontrar regiones \"densas\" de puntos en un espacio de alta dimensión y trata de hacer una representación en un plano 2D (o 3D) de tal manera que las diferentes regiones queden lo más alejadas posible entre ellas y los elementos de cada región lo más cerca entre ellos. La idea es preservar a los que eran los vecinos más cercanos en el plano de mayor dimensión en el espacio de menor dimensión.\n",
    "* Aquí pueden encontrar una explicación completa de sus autores: https://www.youtube.com/watch?v=RJVL80Gg3lA\n",
    "* En sklearn podemos hacer TSNE con la función `TSNE` dentro del módulo `sklearn.manifold`. A esta función le podemos ajustar los parámetros:\n",
    "    * `perplexity`: es un número entero, se recomienda tome un valor de entre 5 y 50. Se ajusta visualmente viendo cuál genera una mejor separación.\n",
    "    * `metric`: La función que mide la distancia entre los puntos.\n",
    "    * `n_components`: el número de dimensiones al que se reducirán los puntos.\n",
    "* Es importante homogenizar la escala de los datos antes de usar TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, perplexity=50, metric='cosine').fit_transform(svd_matrix)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hacemos una copia de los datos e introducimos dos nuevas columnas, que continen los valores obtenidos del TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos_tsne = discursos_cluster.join(pd.DataFrame(X_embedded, columns=['tsne_0', 'tsne_1']))\n",
    "discursos_tsne.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación se grafican todos los documentos en un plano de 2 dimensiones. El color lo asignamos de acuerdo al clúster en el que lo clasificamos en el paso anterior. \n",
    "* Intuitivamente, los documentos que tratan los mismos temas deben estar más cerca unos de otros.\n",
    "* El número de cada punto es el índice del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paleta = list(mcd.XKCD_COLORS.values())[::25] # paleta de colores\n",
    "marcadores = list(markers.MarkerStyle.markers.keys())[1:] # Distintos marcadores\n",
    "ax = plt.subplot()\n",
    "for g in discursos_tsne['grupo'].unique():\n",
    "    discursos_tsne.query('grupo==@g').plot.scatter(x='tsne_0', y='tsne_1', color=paleta[g], marker=marcadores[g], figsize=(10, 8), label=g, ax=ax)\n",
    "\n",
    "for i, row in discursos_tsne.iterrows():\n",
    "    ax.annotate(i, (row.tsne_0, row.tsne_1), fontsize=8)\n",
    "\n",
    "ax.legend(title='grupos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de tópicos con Latent Dirichlet Allocation\n",
    "\n",
    "* Otra tarea muy popular que pueden hacer lo algoritmos de ML con datos de texto es la detección de tópicos. Esta consiste en encontrar tópicos o temas de los que hablan los documentos a partir de su contenido.\n",
    "* Esta es una tarea de aprendizaje no supervisado, por lo que no se requiere de datos de entrenamiento con documentos que ya tengan un tópico asignado, sino que el algoritmo tratará de deducir cuáles son los tópicos.\n",
    "* Nosotros debemos epecificar el número tópicos que queremos extraer.\n",
    "* El algoritmo más usado para esta tarea es el de Latent Dirichlet Allocation (LDA). Este considera cada documento como una composición de varios tópicos, y cada documento es una colección de palabras que también una probabilidad de pertenecer a cada tópico. Los detalles del método están más allá del alacance de este tutorial.\n",
    "* Un tópico es una colección de palabras. Esperaríamos que las palabras asociadas con un tópico tengan están relacionadas y tengan sentido juntas, sin embargo eso no está garantizado. \n",
    "* En sklearn podemos usar LDA con la función `LatentDirichletAllocation` del módulo `sklearn.decomposition`. Especificamos el número de tópicos con la opción `n_componentes`.\n",
    "* Es buena idea limitar el número de palabras para evitar que términos irrelevantes determinen un tópico. En el ejemplo de abajo nos limitamos a 2000 palabras y exluimos a quellas que aparecen en más del 20% de los documentos.\n",
    "* Con la librería[PyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html) se pueden crear visualizaciones interactivas de los modelos LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=2000, max_df=0.2)),\n",
    "    ('lda', LatentDirichletAllocation(n_components=10, max_iter=25, random_state=42))\n",
    "])\n",
    "lda.fit(discursos['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El atributo `.components_` contiene un puntaje para cada palabra y cada tópico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda['lda'].components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creamos un dataframe con el vocabulario y le adjuntamos a cada palabra el púntaje por tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_topic = pd.DataFrame(data=lda['vect'].vocabulary_.items(), columns=['palabra', 'indice'])\\\n",
    "    .sort_values('indice')\\\n",
    "    .set_index('indice')\n",
    "vocab_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "componentes = pd.DataFrame(lda['lda'].components_.T, columns=[f'topico_{i}' for i in range(lda['lda'].components_.shape[0])])\n",
    "componentes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topicos = vocab_topic.join(componentes)\n",
    "df_topicos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el DataFrame resultante podemos saber cómo se compone cada tópico. Las palabras más representativas de un tópico son aquellas que tienen un mayor valor a lo largo de la columna.\n",
    "* Veamos las 20 palabras más representativas de cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top in df_topicos.filter(like='topico').columns:\n",
    "    palabras = df_topicos.sort_values(top).tail(20)[['palabra', top]]\n",
    "    palabras.plot.barh(x='palabra', y=top, title=top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para ver a qué tópico pertenece cada documento podemos usar el método `.transform()` sobre los documentos.\n",
    "* El resultado es una matriz que contiene la probabilidad de que cada documento pertenezca a cada uno de los tópicos.\n",
    "* Un documento se asigna al tópico con mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame(lda.transform(discursos['texto']))\n",
    "doc_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación hacemos una copia del DataFrame original y creamos una nueva columna que contenga el tópico asignado a cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_documento = discursos.copy()\n",
    "topico_documento['topico'] = doc_topic.idxmax(axis=1)\n",
    "topico_documento.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veamos la distribución de los tópicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_documento['topico'].value_counts().plot.bar(title='Distribución de documentos por tópico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
